{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE TECHNIQUES DEMYSTIFIED\n",
    "\n",
    "So you came here, let me guess, it's either you're in a data science competition and you read somewhere about how winners of most competitions always win with ensembles or you're  just a curious data scientist who wants to learn about ensembles.\n",
    "\n",
    "Whichever you are, knowledge of ensembles is one of the most important skill every data scientist/machine learning engineer should have. \n",
    "An ensemble 90 percent of the time would always outperform a single model, and it is the recommended technique for squeezing out more accuracies or lower errors from a machine learning project.\n",
    "\n",
    "If you've done a couple of data science projects, then you have probably used a type of ensemble. Popular algorithms like RandomForest, AdaBoost, XGBoost or CatBoost are different implementations of ensembles.\n",
    "\n",
    "In this article, we will walk through the basic concept of ensembles and you'll learn just enough to construct good ones. So let's begin.\n",
    "\n",
    "##### What you will learn:\n",
    "\n",
    "1. A brief introduction to Ensemble.\n",
    "\n",
    "2. Introduction to the sample data.\n",
    "3. Simple Ensemble Techniques\n",
    "     a. Averaging\n",
    "     b. Weighted Average\n",
    "     c. Max Voting\n",
    "4. Advanced Ensemble Techniques\n",
    "     a. Bagging\n",
    "     b. Boosting\n",
    "     c. Stacking\n",
    "     \n",
    "\n",
    "#### A brief introduction to Ensembles\n",
    "\n",
    "Suppose you want to save money to buy a new laptop, but you do not know how much it sells for and as such cannot set a saving target. Of course, we assume you're out of power and access to the internet because of an alien invasion the previous day.\n",
    "One reasonable thing to do in order to know the price is to ask someone, presumably a friend-we don't want you stopping people on their way to work asking about laptop prices!\n",
    "\n",
    "You find your friend and asks him; he thinks for a moment and mumbles something around $900. Now you know your friend may love tech gadgets, but he certainly does not know the actual price of the laptop. You also know he may not be able to give you the actual price, but all you really care about is that he gives you an estimate close to the true price (Your friend is a single model).\n",
    "\n",
    "Next, you find five of your colleagues from work. Luckily, they are arguing about MAC vs PC for software development. You jump in and pose the question-the price of your dream laptop. Well, as you guessed, they all had a thing or two to say.\n",
    "\n",
    "Person 1 said $1000\n",
    "\n",
    "Person 2 said $950\n",
    "\n",
    "Person 3 said $800\n",
    "\n",
    "Person 4 said $1100\n",
    "\n",
    "Person 5 said $900\n",
    "\n",
    "You look at the following prices from the five different people and notice that they are all within a certain range (900 - 1100). You decided to take the average which is $950. Well, I'm happy to inform you that you just created your first ensemble. A combination of five predictions and taking the average. \n",
    "Surely, you would trust this average better than your one friend right? Yes I would, well, unless your friend is the dealer. \n",
    "\n",
    "That my friend is the intuition behind ensembles, albeit what we illustrated above is just a simple type of ensemble called __averaging__ for regression problems. As we proceed, we'll see other techniques. \n",
    "\n",
    "Formally, [wiki](source:https://en.wikipedia.org/wiki/Ensemble_learning) said an ensemble is a method that uses multiple learning algorithms to obtain better predictive performance than could be obtained by any of the single learning algorithm. \n",
    "\n",
    "\n",
    "#### Introduction to the sample data\n",
    "\n",
    "For this tutorial, we're going to use a [German bank Credit data](). Let's take a peak at the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>loan_duration_mo</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>payment_pcnt_income</th>\n",
       "      <th>time_in_residence</th>\n",
       "      <th>age_yrs</th>\n",
       "      <th>number_loans</th>\n",
       "      <th>dependents</th>\n",
       "      <th>bad_credit</th>\n",
       "      <th>checking_account_status_0 - 200 DM</th>\n",
       "      <th>...</th>\n",
       "      <th>home_ownership_own</th>\n",
       "      <th>home_ownership_rent</th>\n",
       "      <th>job_category_highly skilled</th>\n",
       "      <th>job_category_skilled</th>\n",
       "      <th>job_category_unemployed-unskilled-non-resident</th>\n",
       "      <th>job_category_unskilled-resident</th>\n",
       "      <th>telephone_none</th>\n",
       "      <th>telephone_yes</th>\n",
       "      <th>foreign_worker_no</th>\n",
       "      <th>foreign_worker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1122334</td>\n",
       "      <td>6</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6156361</td>\n",
       "      <td>48</td>\n",
       "      <td>5951</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051359</td>\n",
       "      <td>12</td>\n",
       "      <td>2096</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8740590</td>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3924540</td>\n",
       "      <td>24</td>\n",
       "      <td>4870</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  loan_duration_mo  loan_amount  payment_pcnt_income  \\\n",
       "0      1122334                 6         1169                    4   \n",
       "1      6156361                48         5951                    2   \n",
       "2      2051359                12         2096                    2   \n",
       "3      8740590                42         7882                    2   \n",
       "4      3924540                24         4870                    3   \n",
       "\n",
       "   time_in_residence  age_yrs  number_loans  dependents  bad_credit  \\\n",
       "0                  4       67             2           1           0   \n",
       "1                  2       22             1           1           1   \n",
       "2                  3       49             1           2           0   \n",
       "3                  4       45             1           2           0   \n",
       "4                  4       53             2           2           1   \n",
       "\n",
       "   checking_account_status_0 - 200 DM         ...          home_ownership_own  \\\n",
       "0                                   0         ...                           1   \n",
       "1                                   1         ...                           1   \n",
       "2                                   0         ...                           1   \n",
       "3                                   0         ...                           0   \n",
       "4                                   0         ...                           0   \n",
       "\n",
       "   home_ownership_rent  job_category_highly skilled  job_category_skilled  \\\n",
       "0                    0                            0                     1   \n",
       "1                    0                            0                     1   \n",
       "2                    0                            0                     0   \n",
       "3                    0                            0                     1   \n",
       "4                    0                            0                     1   \n",
       "\n",
       "   job_category_unemployed-unskilled-non-resident  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               0   \n",
       "4                                               0   \n",
       "\n",
       "   job_category_unskilled-resident  telephone_none  telephone_yes  \\\n",
       "0                                0               0              1   \n",
       "1                                0               1              0   \n",
       "2                                1               1              0   \n",
       "3                                0               1              0   \n",
       "4                                0               1              0   \n",
       "\n",
       "   foreign_worker_no  foreign_worker_yes  \n",
       "0                  0                   1  \n",
       "1                  0                   1  \n",
       "2                  0                   1  \n",
       "3                  0                   1  \n",
       "4                  0                   1  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "german_cred = pd.read_csv('credit_preped.csv')\n",
    "german_cred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The data has already been cleaned and all features have been converted to numerical types. The data was original meant to be a classification problem.\n",
    "I.e The task was to predict if a customer's loan application will result in a good or bad credit. \n",
    "\n",
    "Since We'll be explaining ensembles for regression as well as classification tasks, we'll often rephrase the problem using the same data. For classification ensembles, we'll use the feature __bad_credit__ as the target and for regression task, we'll use  __age_yrs__.\n",
    "\n",
    "First, we'll import some modules we'll be using and then we'll drop the customer_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_cred.drop('customer_id', axis=1, inplace=True)\n",
    "\n",
    "#Metric calculations\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set seed\n",
    "rand_seed = 234\n",
    "np.random.seed = rand_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start creating our ensembles, let's train single models and get their performance.\n",
    "For classification we use three simple models: Support Vector Classifiers (SVC), Logistic Regression and K-Nearest Neighbor Classifier.\n",
    "For Regression task, we'll use Linear regression, Support Vector Regressor (SVR) and K-Nearest Neighbor Regressor.\n",
    "\n",
    "We append these models to a list and loop over each as we train and cross validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import single models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "\n",
    "#Classification models\n",
    "log_cf = LogisticRegression(solver='lbfgs', random_state=rand_seed)\n",
    "svc_cf = SVC(gamma='scale', random_state=rand_seed)\n",
    "knn_cf = KNeighborsClassifier()\n",
    "\n",
    "classification_models = [log_cf, svc_cf, knn_cf]\n",
    "\n",
    "#Regression models\n",
    "linear_reg = LinearRegression()\n",
    "svr_reg = SVR(gamma='scale')\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "regression_models = [linear_reg, svr_reg, knn_reg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some functions:\n",
    "The first is used to standardize the data set. \n",
    "\n",
    "The second to split data into train and validation set.\n",
    "\n",
    "The third and fourth measures performance for regression and classification tasks respectively.\n",
    "\n",
    "The final function is used to train and validate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to standardize the data set\n",
    "def standardize_data(df):\n",
    "    scaler = RobustScaler()\n",
    "    data = scaler.fit_transform(df)\n",
    "    return data\n",
    "\n",
    "#Create a function to split our data into train and validation set for both task\n",
    "\n",
    "def get_split_data(features, target_name=None):\n",
    "    ## Get the target column\n",
    "    target = features[target_name]\n",
    "    ## Drop the target from the data\n",
    "    temp_data = features.drop(target_name, axis=1)\n",
    "    temp_data = standardize_data(temp_data)\n",
    "    \n",
    "    #split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(temp_data, target, test_size=0.1)\n",
    "    return (X_train, X_val, y_train, y_val)\n",
    "    \n",
    "def get_mae(pred, true_value):\n",
    "    return mean_absolute_error(true_value, pred)\n",
    "\n",
    "\n",
    "def get_acc(pred, true_value):\n",
    "    return accuracy_score(true_value, pred) * 100\n",
    "\n",
    "# A Function to train and cross validate a model\n",
    "def model_train(model, features=None, target_name=None, nfolds = 10, task = 'class'):\n",
    "    ## Get the target column\n",
    "    target = features[target_name]\n",
    "    ## Drop the target from the data\n",
    "    temp_data = features.drop(target_name, axis=1)\n",
    "    temp_data = standardize_data(temp_data)\n",
    "    \n",
    "    if task == 'reg':\n",
    "        score = -1 * (cross_val_score(model, temp_data, target, cv=nfolds, scoring='neg_mean_absolute_error'))\n",
    "        print(\"Mean Absolute Error of {} is {}\".format(model.__class__.__name__, round(score[0], 4)))\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "    else:\n",
    "        score = cross_val_score(model, temp_data, target, cv=nfolds, scoring='accuracy')\n",
    "        print(\"Accuracy of {} is {} %\".format(model.__class__.__name__, round(score[0] * 100)))\n",
    "        print(\"-------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions, lets test the base models. Remember, we use the feature __bad_credit__ for classification and the feature __age_yrs__ for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression is 82.0 %\n",
      "-------------------------------------\n",
      "Accuracy of SVC is 82.0 %\n",
      "-------------------------------------\n",
      "Accuracy of KNeighborsClassifier is 74.0 %\n",
      "-------------------------------------\n",
      "Mean Absolute Error of LinearRegression is 8.0344\n",
      "-------------------------------------\n",
      "Mean Absolute Error of SVR is 8.8995\n",
      "-------------------------------------\n",
      "Mean Absolute Error of KNeighborsRegressor is 8.508\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Classification \n",
    "for model in classification_models:\n",
    "    model_train(model, features=german_cred, target_name='bad_credit')\n",
    "    \n",
    "#Regression  \n",
    "for model in regression_models:\n",
    "    model_train(model, features=german_cred, target_name='age_yrs', task='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are not trying to compare algorithms in this articles. So we'll not perform heavy hyperparameter tuning and majority of the time we'll just used out-of-box parameters.\n",
    "\n",
    "Now that we have our base models, let's learn about ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Ensemble Techniques\n",
    "\n",
    "#### Averaging\n",
    "\n",
    "Averaging is the simplest and most intuitive type of ensemble for regression task. Just like the name implies, it combines predictions from different models and takes the average/mean.\n",
    "For example, since we're predicting age, if our three base models predicted 24, 23 and 26 respectively, we would take the average as (24 + 23 + 26) / 3 which is approx. 24.3. This becomes our prediction. \n",
    "Let's see this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data sets\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='age_yrs')\n",
    "\n",
    "#fit base models\n",
    "linear_reg.fit(X_train, y_train)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "svr_reg.fit(X_train, y_train)\n",
    "\n",
    "#make predictions with trained models\n",
    "pred1 = linear_reg.predict(X_val)\n",
    "pred2 = knn_reg.predict(X_val)\n",
    "pred3 = svr_reg.predict(X_val)\n",
    "\n",
    "#Take average as final prediction\n",
    "avgpred = (pred1 + pred2 + pred3) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do you think the average prediction does better than the single model? Well, let's find out. We'll calculate the mean absolute error of the individual models and compare with the average.\n",
    "\n",
    "We'll see that the average prediction gives us the lowest MAE and as such does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model\n",
      "6.86126953125\n",
      "KNN Regression Model\n",
      "7.078\n",
      "SVR Regression Model\n",
      "6.9133617373257\n",
      "Average Model\n",
      "6.786586857234646\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression Model\")\n",
    "print(get_mae(pred1, y_val))\n",
    "print(\"KNN Regression Model\")\n",
    "print(get_mae(pred2, y_val))\n",
    "print(\"SVR Regression Model\")\n",
    "print(get_mae(pred3, y_val))\n",
    "print(\"Average Model\")\n",
    "print(get_mae(avgpred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average\n",
    "\n",
    "Weighted Average is a modification of Averaging. The intuition behind this is that some of the base models we want to average may have higher predictive powers than others, as such taking the average may not really capture this individual predictive ability. In cases like this, we assign different weights to different models based on their predictive ability. \n",
    "\n",
    "Note: Weights here simply means decimal numbers that add up to 1\n",
    "\n",
    "Looking at the MAEs of our base regression models above, we see that the Linear Regression model does better than the others, so let's assign a higher weight to it. \n",
    "\n",
    "We demonstrate this below, by assigning 0.5 to the linear model and o.25 to the other two. This can be interpreted as saying \"take the linear model 50% more serious than the other two\"\n",
    "\n",
    "We'll observe that the weighted average does better, even better than (slightly though) than the averaging ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model\n",
      "6.86126953125\n",
      "KNN Regression Model\n",
      "7.078\n",
      "SVR Regression Model\n",
      "6.9133617373257\n",
      "Weighted Average Model\n",
      "6.753724322613486\n"
     ]
    }
   ],
   "source": [
    "#fit base models\n",
    "linear_reg.fit(X_train, y_train)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "svr_reg.fit(X_train, y_train)\n",
    "\n",
    "#make predictions with trained models\n",
    "pred1 = linear_reg.predict(X_val)\n",
    "pred2 = knn_reg.predict(X_val)\n",
    "pred3 = svr_reg.predict(X_val)\n",
    "\n",
    "#Take average as final prediction\n",
    "w_avgpred = (0.5 * pred1 + 0.25 * pred2 + 0.25* pred3)\n",
    "\n",
    "print(\"Linear Regression Model\")\n",
    "print(get_mae(pred1, y_val))\n",
    "print(\"KNN Regression Model\")\n",
    "print(get_mae(pred2, y_val))\n",
    "print(\"SVR Regression Model\")\n",
    "print(get_mae(pred3, y_val))\n",
    "print(\"Weighted Average Model\")\n",
    "print(get_mae(w_avgpred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Voting\n",
    "\n",
    "The Max Voting is similar to averaging except it is used for classification problems. In max voting as the name implies, we train multiple models, make predictions and then take the maximum/modal/most popular class as the predicted class. \n",
    "\n",
    "Let's understand this better with an example; suppose you ask your friends to rate the laptop you are saving up for.\n",
    "\n",
    "On a scale of 1 to 5, your friends rated as follow:\n",
    "\n",
    "Friend 1 = 3\n",
    "\n",
    "Friend 2 = 5\n",
    "\n",
    "Friend 3 = 4\n",
    "\n",
    "Friend 4 = 3\n",
    "\n",
    "Friend 5 = 3\n",
    "\n",
    "Now, looking at the ratings, if we use max voting, we simply pick the rating that occur most which is 5. \n",
    "\n",
    "We demonstrate this in code below.\n",
    "\n",
    "Note: we are now working on a classification task, so we're predicting __bad_credit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "70.0\n",
      "KNN Classifier Model\n",
      "69.0\n",
      "SVR Classifier Model\n",
      "71.0\n",
      "Max Voting Model\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "#get the data sets\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='bad_credit')\n",
    "\n",
    "#fit single models\n",
    "log_cf.fit(X_train, y_train)\n",
    "knn_cf.fit(X_train, y_train)\n",
    "svc_cf.fit(X_train, y_train)\n",
    "\n",
    "#make predictions with trained models\n",
    "pred1 = log_cf.predict(X_val)\n",
    "pred2 = knn_cf.predict(X_val)\n",
    "pred3 = svc_cf.predict(X_val)\n",
    "\n",
    "#Take max voting as final prediction\n",
    "maxpred = []\n",
    "\n",
    "for i in range(0, len(X_val)):\n",
    "    #calculate the mode and append to maxpred vector\n",
    "    maxpred.append(mode([pred1[i], pred2[i], pred3[i]]))\n",
    "    \n",
    "    \n",
    "print(\"Logistic Regression Model\")\n",
    "print(get_acc(pred1, y_val))\n",
    "print(\"KNN Classifier Model\")\n",
    "print(get_acc(pred2, y_val))\n",
    "print(\"SVR Classifier Model\")\n",
    "print(get_acc(pred3, y_val))\n",
    "\n",
    "print(\"Max Voting Model\")\n",
    "print(get_acc(np.array(maxpred), y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of it is worth mentioning that sklearn has an implementation of Max Voting (VotingClassifier) that you can use. An example of using this module is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Voting in sklearn\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "#Import the module\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Pass the classifiers as a list of tuples with model names and the models themselves\n",
    "max_model = VotingClassifier(estimators=[('logistic_reg', log_cf), ('KNN Classifier', knn_cf), (\"SVC\", svc_cf)], voting='hard')\n",
    "max_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Max Voting in sklearn\")\n",
    "print(get_acc(max_model.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Ensemble Techniques\n",
    "\n",
    "##### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind bagging (Bootstrap Aggregating) is quite simple. It is similar to averaging except one tiny change which is made on the data set we use to train the model.\n",
    "\n",
    "In averaging we train multiple models on the same data set and take the average, but in bagging we train the models on different sub-samples of the orignal dataset before taking the combined predictions.\n",
    "\n",
    "One question you may ask is that if we train on subsets of a data set, are we not still training on the same dataset? will the result not be similar?\n",
    "The answer is actually a gray 'no\". We are not training on the exactly the same dataset, yes the sub-samples are similar, but about 30% of the time we get different observations in them.\n",
    "\n",
    "More on why bagging works can be found in this [paper]()\n",
    "\n",
    "While it is possible to write your own bagging algorithm, it is advisable to use already existing bagging implementations. \n",
    "Many popular algorithms like RandomForest and ExtraTrees are implementations of bagging. \n",
    "\n",
    "In sklearn library, it is possible to also create your own bagging classifier or regressor from a specified base models. \n",
    "\n",
    "Let's see this in code below.\n",
    "First, we import some bagging implementations in sklearn, we also import the bagging meta-estimator; this allows us to choose our own base model.\n",
    "\n",
    "Note: As earlier said, the goal of the tutorial is to teach you the techniques of ensembling, not how to fine tune the models. So the MAEs or Accuracy may be high or low depending on the default parameter we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging and Boosting models for both classification and regression problems\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "#import xgboost as xgb\n",
    "\n",
    "\n",
    "#bagging algorithms for regression\n",
    "rand_forest_reg = RandomForestRegressor(n_estimators=100, random_state=rand_seed)\n",
    "extra_tree_reg = ExtraTreesRegressor(n_estimators=100,random_state=rand_seed)\n",
    "#We use linear regressor as our base model for bagging\n",
    "bagging_meta_reg = BaggingRegressor(svr_reg, n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "#bagging algorithms for classification\n",
    "rand_forest_cf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "extra_tree_cf = ExtraTreesClassifier(n_estimators=100, random_state=rand_seed)\n",
    "#We use svc as our base model for bagging\n",
    "bagging_meta_cf = BaggingClassifier(svc_cf, n_estimators=10, random_state=rand_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get our train and validation set for regression task, train the model and then test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Random Forest is :  6.6429\n",
      "MAE of Extra Trees is :  7.4214\n",
      "MAE of Bagging estimator is :  6.29636897349609\n"
     ]
    }
   ],
   "source": [
    "#get data for regression task\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='age_yrs')\n",
    "\n",
    "#Train and fit these models\n",
    "rand_forest_reg.fit(X_train, y_train)\n",
    "extra_tree_reg.fit(X_train, y_train)\n",
    "bagging_meta_reg.fit(X_train, y_train)\n",
    "\n",
    "#check their performance\n",
    "print(\"MAE of Random Forest is : \", get_mae(rand_forest_reg.predict(X_val), y_val))\n",
    "print(\"MAE of Extra Trees is : \", get_mae(extra_tree_reg.predict(X_val), y_val))\n",
    "print(\"MAE of Bagging estimator is : \", get_mae(bagging_meta_reg.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We do the same for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Random Forest is :  74.0\n",
      "MAE of Extra Trees is :  72.0\n",
      "MAE of Bagging estimator is :  75.0\n"
     ]
    }
   ],
   "source": [
    "#get data for classification task\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='bad_credit')\n",
    "\n",
    "#Train and fit these models\n",
    "rand_forest_cf.fit(X_train, y_train)\n",
    "extra_tree_cf.fit(X_train, y_train)\n",
    "bagging_meta_cf.fit(X_train, y_train)\n",
    "\n",
    "#check their performance\n",
    "print(\"ACC of Random Forest is : \", get_acc(rand_forest_cf.predict(X_val), y_val))\n",
    "print(\"ACC of Extra Trees is : \", get_acc(extra_tree_cf.predict(X_val), y_val))\n",
    "print(\"ACC of Bagging estimator is : \", get_acc(bagging_meta_cf.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "\n",
    "Boosting is another popular and effective ensembling technique. In Boosting, multiple models are trained sequentially. The goal is to train models that do better than their predecessors. This means we have to take into account the areas where the previous models performed poorly and improve on those area. If we keep doing this-improving on the failures of the predecessors, theoritically, it can be shown that we'll acheive a perfect model. But the world is never perfect and as such this may not be achievable in practice. \n",
    "Boosting works really well and is definitely some of the go to algorithms when doing data science competitions.\n",
    "\n",
    "There exist many implementation of Boosting some of which are XGBoost, LigthGBM, AdaBoost, CatBoost etc.\n",
    "\n",
    "Let's see some implementation of these boosting algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import boosting regressoion algorithms\n",
    "# import xgboost.XGBRegressor as xgb_reg\n",
    "# import lightgbm.LGBRegressor as lgb_reg\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "#Import boosting regressoion algorithms\n",
    "# import xgboost.XGBClassifier as xgb_cf\n",
    "# import lightgbm.LGBClassifier as lgb_cf\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "#Regression\n",
    "ada_reg = AdaBoostRegressor(base_estimator=svr_reg,n_estimators=100, random_state=rand_seed)\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "#Classification\n",
    "ada_cf = AdaBoostClassifier(base_estimator=log_cf, random_state=rand_seed)\n",
    "gb_cf = GradientBoostingClassifier(random_state=rand_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialize our boosting algorithms, we simply train and calculate performance on both regression and classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of AdaBoost is :  7.366257179997746\n",
      "MAE of Gradient Boosting is :  6.850286630903513\n",
      "ACC of AdaBoost is :  77.0\n",
      "ACC of Gradient Boosting is :  78.0\n"
     ]
    }
   ],
   "source": [
    "#get data for regression task\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='age_yrs')\n",
    "\n",
    "#Train and fit these models\n",
    "ada_reg.fit(X_train, y_train)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "#check their performance\n",
    "print(\"MAE of AdaBoost is : \", get_mae(ada_reg.predict(X_val), y_val))\n",
    "print(\"MAE of Gradient Boosting is : \", get_mae(gb_reg.predict(X_val), y_val))\n",
    "\n",
    "\n",
    "#get data for regression task\n",
    "X_train, X_val, y_train, y_val = get_split_data(german_cred, target_name='bad_credit')\n",
    "\n",
    "#Train and fit these models\n",
    "ada_cf.fit(X_train, y_train)\n",
    "gb_cf.fit(X_train, y_train)\n",
    "\n",
    "#check their performance\n",
    "print(\"ACC of AdaBoost is : \", get_acc(ada_cf.predict(X_val), y_val))\n",
    "print(\"ACC of Gradient Boosting is : \", get_acc(gb_cf.predict(X_val), y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of the box without tuning hyperparameters, we can see that boosting performed better in the classification task than both bagging and single models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "\n",
    "\n",
    "Stacking is an advance ensemble technique that has also been proven to give higher performance. Stacking is almost always behind the success of most data science competitions on kaggle.\n",
    "\n",
    "In Stacking approach, we simply train and make predictions on the original training data set using the base algorithms which are called first-level learners. The predictions from these base learners are then combined to make up a new training data\n",
    "for another algorithm called a meta-learner. I.e. The output of the first level learners serves as input for the meta-learner.\n",
    "\n",
    "The first level learners are often made up of different, simple and diverse algorithms although it is possible to create stacked ensembles from the same learning algorithms. \n",
    "\n",
    "The procedure, for the satcking is may be described as follows:\n",
    "\n",
    "\n",
    "1. Split the total training set into two disjoint sets (here **train** and .**test** )\n",
    "\n",
    "2. Train several base models on the first part (**train**)\n",
    "\n",
    "3. Test these base models on the second part (**test**)\n",
    "\n",
    "4. Use the predictions from 3) as the inputs, and the correct responses (target) as the outputs  to train a higher level learner called **meta-model**.\n",
    "\n",
    "The first three steps are done iteratively . If we take for example a 10-fold stacking , we first split the training data into 10 folds. Then we will do 10 iterations. In each iteration,  we train every base model on 9 folds and predict on the remaining fold (holdout fold). \n",
    "\n",
    "So, we will be sure, after 10 iterations , that the entire data is used to get test predictions which we use as \n",
    "new feature to train our meta-model in the step 4.\n",
    "\n",
    "For the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n",
    "\n",
    "You do not need to code a stacking ensemble yourself as there alredy exist many efficient implementation of it. Some popular implementations are [ML Ensemble](http://ml-ensemble.com/), [H20](https://en.wikipedia.org/wiki/H2O_(software)).\n",
    "__NOTE:__ It is advisable to use more efficient implementations of a stacking ensembles.\n",
    "\n",
    "In this article we'll write our own simple stacking ensemble just to demonstrate the idea. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def stackingModel(base_models, meta_model, features, target, nfolds=10):\n",
    "    #Split data into folds\n",
    "    kfold = KFold(n_splits=nfolds, shuffle=True, random_state=rand_seed)\n",
    "    #initialize arrays to hold predictions\n",
    "    test_predictions = np.zeros((features.shape[0], len(base_models)))\n",
    "    train_predictions = np.zeros((features.shape[0], len(base_models)))\n",
    "    \n",
    "    # Train base models\n",
    "    for i, model in enumerate(base_models):\n",
    "        for train_index, test_index in kfold.split(features, target):\n",
    "            #Fit train data on the model\n",
    "            model.fit(np.array(features)[train_index], np.array(target)[train_index])\n",
    "            \n",
    "            #Make prediction on the holdout data\n",
    "            y_pred = model.predict(np.array(features)[test_index])\n",
    "            #make predictions on train data\n",
    "            t_pred = model.predict(np.array(features)[train_index])\n",
    "            \n",
    "            #Append the prediction to out of folds\n",
    "            test_predictions[test_index, i] = y_pred\n",
    "            #Append predictions to train predictions\n",
    "            train_predictions[train_index, i] = t_pred\n",
    "\n",
    "\n",
    "    # Now train the meta-model using the train predictions as new feature\n",
    "    meta_model.fit(train_predictions, target)\n",
    "    #Make fianl predictions on the average of out of fold predictions\n",
    "    final_preds = meta_model.predict(np.mean([test_predictions], axis=0))\n",
    "    \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a simple stacking ensemble, lets train and test on a regression task first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Stacking Model is :  7.23564330035632\n",
      "ACC of Stacking Model is :  74.9\n"
     ]
    }
   ],
   "source": [
    "#get data for regression task\n",
    "target = german_cred['age_yrs']\n",
    "data = german_cred.drop('age_yrs', axis=1)\n",
    "data = standardize_data(data)\n",
    "\n",
    "#first level learners\n",
    "base_learners = [linear_reg, svr_reg, knn_reg]\n",
    "#meta learner\n",
    "meta_ln = svr_reg\n",
    "\n",
    "pred = stackingModel(base_learners, meta_ln, data, target)\n",
    "\n",
    "#check performance\n",
    "print(\"MAE of Stacking Model is : \", get_mae(pred ,target))\n",
    "\n",
    "\n",
    "#get data for classification task\n",
    "target = german_cred['bad_credit']\n",
    "data = german_cred.drop('bad_credit', axis=1)\n",
    "data = standardize_data(data)\n",
    "\n",
    "#first level learners\n",
    "base_learners = [log_cf, svc_cf, knn_cf]\n",
    "#meta learner\n",
    "meta_ln = svc_cf\n",
    "\n",
    "pred = stackingModel(base_learners, meta_ln, data, target)\n",
    "\n",
    "#check performance\n",
    "print(\"ACC of Stacking Model is : \", get_acc(pred ,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the stacking ensemble above, we created just 2 levels; \n",
    "\n",
    "level 1 for the base models and level 2 for the meta-model. \n",
    "\n",
    "You can create as many levels as you wish but make sure tyhe models are well diversed as stacking performs better on diverse set of base learners.\n",
    "\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Ensembles are tried and tested methods for greatly improving the performance of your machine learning models and most times is the difference between 1st and second place in a data science competition. In this article, we have covered some of the basic ideas beind ensembles. It is worth mentionng here that we can combine ensembles together to create more complex ensembles. While this may help sometimes, most times the performance drops. Remember the \"No Free Lunch Rule\"? Well that happens.\n",
    "\n",
    "I'm sure this article has given you a solid background in ensembles, please clap and share. If you have any questions, suggestions or corrections, do share in the comment section below.\n",
    "\n",
    "[LINK](https://github.com/mediumtutorial) to Notebook on Github. Don't forget to leave a star if this helped you.\n",
    "\n",
    "[LINK](https://www.academia.edu/39060549/An_Empirical_Study_of_Ensemble_Techniques_Bagging_Boosting_and_Stacking?source=swp_share) to my paper on ensemble techniques.\n",
    "\n",
    "CONNECT WITH ME: \n",
    "TWITTER: @risin_developer\n",
    "Linkedin: https://www.linkedin.com/in/risingdeveloper\n",
    "\n",
    "HAPPY CODING!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
